# Penjelasan Matematika Model RNN, LSTM, dan GRU

Dokumen ini memberikan penjelasan matematika dari tiga arsitektur jaringan saraf berulang yang digunakan dalam tugas analisis sentimen: RNN Sederhana (Simple RNN), LSTM (Long Short-Term Memory), dan GRU (Gated Recurrent Unit).

## 1. RNN Sederhana (Simple RNN)

### Unit Berulang Dasar
RNN Sederhana memproses sekuens input dan mempertahankan keadaan tersembunyi (hidden state) yang diperbarui pada setiap langkah waktu. Untuk setiap langkah waktu $t$, keadaan tersembunyi $h_t$ dihitung sebagai:

$$h_t = \sigma(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$

Dimana:
- $x_t$ adalah input pada langkah waktu $t$
- $h_{t-1}$ adalah keadaan tersembunyi dari langkah waktu sebelumnya
- $W_{xh}$ adalah matriks bobot untuk koneksi input-ke-tersembunyi
- $W_{hh}$ adalah matriks bobot untuk koneksi berulang tersembunyi-ke-tersembunyi
- $b_h$ adalah term bias
- $\sigma$ adalah fungsi aktivasi (biasanya tanh atau ReLU)

Untuk output, kita menggunakan keadaan tersembunyi terakhir:

$$y = \sigma(W_{hy} h_T + b_y)$$

Dimana:
- $h_T$ adalah keadaan tersembunyi pada langkah waktu terakhir
- $W_{hy}$ adalah matriks bobot untuk koneksi tersembunyi-ke-output
- $b_y$ adalah bias output
- $\sigma$ adalah fungsi aktivasi (sigmoid untuk klasifikasi biner)

### Masalah Gradien yang Menghilang/Meledak
RNN sederhana menderita masalah gradien yang menghilang atau meledak selama backpropagation melalui waktu. Saat menghitung gradien, kita mengalikan dengan $W_{hh}$ berulang kali:

$$\frac{\partial L}{\partial h_t} = \frac{\partial L}{\partial h_{t+1}} \cdot \frac{\partial h_{t+1}}{\partial h_t} = \frac{\partial L}{\partial h_{t+1}} \cdot W_{hh} \cdot \text{diag}(\sigma'(W_{xh} x_t + W_{hh} h_{t-1} + b_h))$$

Jika nilai eigen dari $W_{hh}$ kurang dari 1, gradien menghilang pada sekuens panjang. Jika lebih besar dari 1, gradien meledak.

## 2. LSTM (Long Short-Term Memory)

LSTM menyelesaikan masalah gradien yang menghilang dengan memperkenalkan keadaan sel dan tiga mekanisme gerbang:

### Gerbang dan Keadaan Sel
Untuk setiap langkah waktu $t$:

1. **Gerbang Lupa (Forget Gate)**: Memutuskan informasi apa yang akan dibuang dari keadaan sel
   $$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

2. **Gerbang Input**: Memutuskan informasi baru apa yang akan disimpan dalam keadaan sel
   $$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
   $$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

3. **Pembaruan Keadaan Sel**: Memperbarui keadaan sel lama menjadi keadaan sel baru
   $$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

4. **Gerbang Output**: Memutuskan bagian mana dari keadaan sel yang akan dioutputkan
   $$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
   $$h_t = o_t \odot \tanh(C_t)$$

Dimana:
- $\odot$ merepresentasikan perkalian elemen-wise
- $[h_{t-1}, x_t]$ merepresentasikan penggabungan keadaan tersembunyi sebelumnya dan input saat ini
- $\sigma$ adalah fungsi aktivasi sigmoid
- $W_f, W_i, W_C, W_o$ adalah matriks bobot
- $b_f, b_i, b_C, b_o$ adalah term bias

Keadaan sel $C_t$ bertindak sebagai jalan raya yang dapat membawa informasi melintasi banyak langkah waktu dengan perubahan minimal, memungkinkan jaringan untuk mempelajari dependensi jangka panjang.

### LSTM Dua Arah (Bidirectional LSTM)

Dalam implementasi kita, kita menggunakan LSTM dua arah yang memproses sekuens input dalam arah maju dan mundur:

$$\overrightarrow{h_t} = \text{LSTM}_{\text{maju}}(x_t, \overrightarrow{h_{t-1}})$$
$$\overleftarrow{h_t} = \text{LSTM}_{\text{mundur}}(x_t, \overleftarrow{h_{t+1}})$$
$$h_t = [\overrightarrow{h_t}, \overleftarrow{h_t}]$$

Ini memungkinkan model untuk menangkap konteks dari kedua keadaan masa lalu dan masa depan untuk langkah waktu tertentu.

## 3. GRU (Gated Recurrent Unit)

GRU adalah versi yang disederhanakan dari LSTM dengan parameter yang lebih sedikit:

### Gerbang Pembaruan dan Reset

Untuk setiap langkah waktu $t$:

1. **Gerbang Pembaruan (Update Gate)**: Memutuskan berapa banyak informasi masa lalu yang akan disimpan
   $$z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$$

2. **Gerbang Reset**: Memutuskan berapa banyak informasi masa lalu yang akan dilupakan
   $$r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$$

3. **Keadaan Tersembunyi Kandidat**: Menghitung keadaan tersembunyi kandidat
   $$\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)$$

4. **Pembaruan Keadaan Tersembunyi**: Memperbarui keadaan tersembunyi
   $$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$

Dimana:
- $\odot$ merepresentasikan perkalian elemen-wise
- $[h_{t-1}, x_t]$ merepresentasikan penggabungan keadaan tersembunyi sebelumnya dan input saat ini
- $\sigma$ adalah fungsi aktivasi sigmoid
- $W_z, W_r, W_h$ adalah matriks bobot
- $b_z, b_r, b_h$ adalah term bias

GRU menggabungkan gerbang lupa dan input menjadi satu gerbang pembaruan, dan menggabungkan keadaan sel dan keadaan tersembunyi. Ini membuatnya lebih efisien secara komputasi sambil tetap mengatasi masalah gradien yang menghilang.

## 4. Metrik Evaluasi

### Loss Binary Cross-Entropy
Untuk klasifikasi biner, kita menggunakan loss binary cross-entropy:

$$L = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]$$

Dimana:
- $N$ adalah jumlah sampel
- $y_i$ adalah label sebenarnya (0 atau 1)
- $\hat{y}_i$ adalah probabilitas yang diprediksi

### Akurasi (Accuracy)
Proporsi instance yang diklasifikasikan dengan benar:

$$\text{Akurasi} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}$$

### Presisi (Precision)
Proporsi prediksi positif benar di antara semua prediksi positif:

$$\text{Presisi} = \frac{\text{TP}}{\text{TP} + \text{FP}}$$

### Recall
Proporsi prediksi positif benar di antara semua positif aktual:

$$\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$$

### F1-Score
Rata-rata harmonik dari presisi dan recall:

$$\text{F1} = 2 \times \frac{\text{Presisi} \times \text{Recall}}{\text{Presisi} + \text{Recall}}$$

### AUC-ROC
Area di Bawah Kurva Karakteristik Operasi Penerima (Receiver Operating Characteristic). Kurva ROC memplot Tingkat Positif Benar (Recall) terhadap Tingkat Positif Palsu:

$$\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}$$
$$\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}$$

AUC mengukur area di bawah kurva ini, dengan nilai yang lebih tinggi menunjukkan diskriminasi yang lebih baik.

## 5. Fitur Model Tambahan

### Lapisan Embedding
Mentransformasi indeks kata menjadi vektor padat:

$$e_t = W_e x_t$$

Dimana:
- $x_t$ adalah vektor one-hot encoded atau indeks
- $W_e$ adalah matriks embedding
- $e_t$ adalah vektor embedding yang dihasilkan

### Normalisasi Lapisan (Layer Normalization)
Menormalkan aktivasi dari lapisan sebelumnya untuk memiliki rata-rata nol dan varians satu:

$$\mu = \frac{1}{H} \sum_{i=1}^{H} a_i$$
$$\sigma^2 = \frac{1}{H} \sum_{i=1}^{H} (a_i - \mu)^2$$
$$\hat{a}_i = \frac{a_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$$
$$b_i = \gamma \hat{a}_i + \beta$$

Dimana:
- $a_i$ adalah aktivasi dari neuron ke-i
- $H$ adalah jumlah neuron dalam lapisan
- $\gamma$ dan $\beta$ adalah parameter yang dipelajari
- $\epsilon$ adalah nilai kecil untuk stabilitas numerik
