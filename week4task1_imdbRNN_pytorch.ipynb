{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMS+BdXHWIZ4b1IOXb9xF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rizkyprofs/ML_DL/blob/main/week4task1_imdbRNN_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ccuI75Xoixi",
        "outputId": "dabb8ad9-007a-40b3-bad6-b0fff0c01f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
        "import pandas as pd\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "NUM_WORDS = 40000  # Vocabulary size\n",
        "MAXLEN = 400       # Maximum review length\n",
        "EMBEDDING_DIM = 128\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# Load the IMDb dataset\n",
        "print(\"Loading IMDB dataset...\")\n",
        "(X_train_full, y_train_full), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "X_train_full = pad_sequences(X_train_full, maxlen=MAXLEN)\n",
        "X_test = pad_sequences(X_test, maxlen=MAXLEN)\n",
        "\n",
        "# Split train into train and validation\n",
        "val_size = int(VALIDATION_SPLIT * len(X_train_full))\n",
        "X_train, X_val = X_train_full[val_size:], X_train_full[:val_size]\n",
        "y_train, y_val = y_train_full[val_size:], y_train_full[:val_size]\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SciCTMfgo4Nw",
        "outputId": "4257b428-d750-4a6d-82cb-45e6a9d3af28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training data shape: (20000, 400)\n",
            "Validation data shape: (5000, 400)\n",
            "Testing data shape: (25000, 400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PyTorch dataset\n",
        "class IMDbDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.LongTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataset = IMDbDataset(X_train, y_train)\n",
        "val_dataset = IMDbDataset(X_val, y_val)\n",
        "test_dataset = IMDbDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "nJ65-3RDo5gT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RNN Model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim=128, output_dim=1, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=0.3, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        # Use the output of the last time step from both directions\n",
        "        output = output[:, -1, :]\n",
        "        output = self.layer_norm(output)\n",
        "        output = self.dropout(output)\n",
        "        output = self.relu(self.fc1(output))\n",
        "        output = self.dropout(output)\n",
        "        output = self.relu(self.fc2(output))\n",
        "        output = self.fc3(output)\n",
        "        return self.sigmoid(output)\n",
        "\n",
        "# Define LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim=128, output_dim=1, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=0.3, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        # Concatenate the final forward and backward hidden states\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        hidden = self.layer_norm(hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "        output = self.relu(self.fc1(hidden))\n",
        "        output = self.dropout(output)\n",
        "        output = self.relu(self.fc2(output))\n",
        "        output = self.dropout(output)\n",
        "        output = self.relu(self.fc3(output))\n",
        "        output = self.fc4(output)\n",
        "        return self.sigmoid(output)\n",
        "\n",
        "# Define GRU Model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim=128, output_dim=1, dropout=0.5):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=0.3, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.gru(embedded)\n",
        "        # Concatenate the final forward and backward hidden states\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        hidden = self.layer_norm(hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "        output = self.relu(self.fc1(hidden))\n",
        "        output = self.dropout(output)\n",
        "        output = self.relu(self.fc2(output))\n",
        "        output = self.dropout(output)\n",
        "        output = self.relu(self.fc3(output))\n",
        "        output = self.fc4(output)\n",
        "        return self.sigmoid(output)"
      ],
      "metadata": {
        "id": "Q4ahCq4Xo-b4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, model_name):\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        # Training loop\n",
        "        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data).squeeze()\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_total += target.size(0)\n",
        "            predicted = (output >= 0.5).float()\n",
        "            train_correct += (predicted == target).sum().item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data).squeeze()\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_total += target.size(0)\n",
        "                predicted = (output >= 0.5).float()\n",
        "                val_correct += (predicted == target).sum().item()\n",
        "\n",
        "        # Calculate average losses and accuracies\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        train_accuracy = train_correct / train_total\n",
        "        val_accuracy = val_correct / val_total\n",
        "\n",
        "        # Save metrics\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), f\"{model_name}_best.pt\")\n",
        "            print(f\"Model saved as {model_name}_best.pt\")\n",
        "\n",
        "        # Print progress\n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
        "        print(f'\\tTrain Loss: {avg_train_loss:.3f} | Train Acc: {train_accuracy:.3f}')\n",
        "        print(f'\\tVal. Loss: {avg_val_loss:.3f} | Val. Acc: {val_accuracy:.3f}')\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title(f'{model_name} - Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Training Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} - Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{model_name}_training_history_pytorch.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return model, {'train_losses': train_losses, 'val_losses': val_losses,\n",
        "                  'train_accs': train_accs, 'val_accs': val_accs}"
      ],
      "metadata": {
        "id": "F3ngvlHlpEDz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader, model_name):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    all_predictions = []\n",
        "    all_probabilities = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data).squeeze()\n",
        "            predicted = (output >= 0.5).float()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_probabilities.extend(output.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_probabilities = np.array(all_probabilities)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_targets, all_predictions)\n",
        "    precision = precision_score(all_targets, all_predictions)\n",
        "    recall = recall_score(all_targets, all_predictions)\n",
        "    f1 = f1_score(all_targets, all_predictions)\n",
        "\n",
        "    # Calculate ROC curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(all_targets, all_probabilities)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve - {model_name} (PyTorch)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(f\"{model_name}_roc_curve_pytorch.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(all_targets, all_predictions)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "    plt.title(f'Confusion Matrix - {model_name} (PyTorch)')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig(f\"{model_name}_confusion_matrix_pytorch.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Return all metrics\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': roc_auc,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr\n",
        "    }"
      ],
      "metadata": {
        "id": "iZnReZqMpJ3f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the models\n",
        "rnn_model = RNNModel(NUM_WORDS, EMBEDDING_DIM)\n",
        "lstm_model = LSTMModel(NUM_WORDS, EMBEDDING_DIM)\n",
        "gru_model = GRUModel(NUM_WORDS, EMBEDDING_DIM)\n",
        "\n",
        "# Loss function and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)\n",
        "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)\n",
        "gru_optimizer = optim.Adam(gru_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Dictionary to store all results\n",
        "all_results = {}"
      ],
      "metadata": {
        "id": "l7Spk-qApQPh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate RNN model\n",
        "print(\"\\n=== Training RNN Model ===\")\n",
        "trained_rnn_model, rnn_history = train_model(\n",
        "    rnn_model, train_loader, val_loader, criterion,\n",
        "    rnn_optimizer, EPOCHS, \"RNN_PyTorch\"\n",
        ")\n",
        "rnn_results = evaluate_model(trained_rnn_model, test_loader, \"RNN_PyTorch\")\n",
        "all_results['RNN'] = rnn_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHgugMoJpVWi",
        "outputId": "28f79ec3-3331-427a-c789-ceb041f2a5ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training RNN Model ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 157/157 [00:05<00:00, 29.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as RNN_PyTorch_best.pt\n",
            "Epoch: 01 | Time: 0.0m 5.77s\n",
            "\tTrain Loss: 0.672 | Train Acc: 0.570\n",
            "\tVal. Loss: 0.626 | Val. Acc: 0.645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 157/157 [00:04<00:00, 38.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as RNN_PyTorch_best.pt\n",
            "Epoch: 02 | Time: 0.0m 4.44s\n",
            "\tTrain Loss: 0.598 | Train Acc: 0.676\n",
            "\tVal. Loss: 0.615 | Val. Acc: 0.672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 157/157 [00:04<00:00, 37.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as RNN_PyTorch_best.pt\n",
            "Epoch: 03 | Time: 0.0m 4.50s\n",
            "\tTrain Loss: 0.553 | Train Acc: 0.723\n",
            "\tVal. Loss: 0.565 | Val. Acc: 0.729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 157/157 [00:04<00:00, 38.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as RNN_PyTorch_best.pt\n",
            "Epoch: 04 | Time: 0.0m 4.45s\n",
            "\tTrain Loss: 0.550 | Train Acc: 0.712\n",
            "\tVal. Loss: 0.544 | Val. Acc: 0.733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 157/157 [00:04<00:00, 38.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Time: 0.0m 4.46s\n",
            "\tTrain Loss: 0.464 | Train Acc: 0.790\n",
            "\tVal. Loss: 0.555 | Val. Acc: 0.743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 157/157 [00:04<00:00, 37.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Time: 0.0m 4.46s\n",
            "\tTrain Loss: 0.487 | Train Acc: 0.770\n",
            "\tVal. Loss: 0.635 | Val. Acc: 0.624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 157/157 [00:04<00:00, 38.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Time: 0.0m 4.43s\n",
            "\tTrain Loss: 0.542 | Train Acc: 0.724\n",
            "\tVal. Loss: 0.575 | Val. Acc: 0.743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 157/157 [00:04<00:00, 37.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Time: 0.0m 4.51s\n",
            "\tTrain Loss: 0.465 | Train Acc: 0.787\n",
            "\tVal. Loss: 0.593 | Val. Acc: 0.677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 157/157 [00:04<00:00, 38.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as RNN_PyTorch_best.pt\n",
            "Epoch: 09 | Time: 0.0m 4.48s\n",
            "\tTrain Loss: 0.409 | Train Acc: 0.825\n",
            "\tVal. Loss: 0.510 | Val. Acc: 0.776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 157/157 [00:04<00:00, 37.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Time: 0.0m 4.44s\n",
            "\tTrain Loss: 0.408 | Train Acc: 0.822\n",
            "\tVal. Loss: 0.534 | Val. Acc: 0.755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 196/196 [00:01<00:00, 123.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate LSTM model\n",
        "print(\"\\n=== Training LSTM Model ===\")\n",
        "trained_lstm_model, lstm_history = train_model(\n",
        "    lstm_model, train_loader, val_loader, criterion,\n",
        "    lstm_optimizer, EPOCHS, \"LSTM_PyTorch\"\n",
        ")\n",
        "lstm_results = evaluate_model(trained_lstm_model, test_loader, \"LSTM_PyTorch\")\n",
        "all_results['LSTM'] = lstm_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-XnV2IepY5H",
        "outputId": "a09f1cc6-2500-4998-b259-f41e371c20cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training LSTM Model ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 157/157 [00:14<00:00, 10.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as LSTM_PyTorch_best.pt\n",
            "Epoch: 01 | Time: 0.0m 16.01s\n",
            "\tTrain Loss: 0.672 | Train Acc: 0.563\n",
            "\tVal. Loss: 0.606 | Val. Acc: 0.678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 157/157 [00:15<00:00, 10.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as LSTM_PyTorch_best.pt\n",
            "Epoch: 02 | Time: 0.0m 16.42s\n",
            "\tTrain Loss: 0.555 | Train Acc: 0.728\n",
            "\tVal. Loss: 0.493 | Val. Acc: 0.761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 157/157 [00:15<00:00, 10.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as LSTM_PyTorch_best.pt\n",
            "Epoch: 03 | Time: 0.0m 16.74s\n",
            "\tTrain Loss: 0.458 | Train Acc: 0.792\n",
            "\tVal. Loss: 0.434 | Val. Acc: 0.799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 157/157 [00:15<00:00,  9.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Time: 0.0m 17.11s\n",
            "\tTrain Loss: 0.424 | Train Acc: 0.818\n",
            "\tVal. Loss: 0.507 | Val. Acc: 0.773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 157/157 [00:15<00:00,  9.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Time: 0.0m 17.16s\n",
            "\tTrain Loss: 0.375 | Train Acc: 0.842\n",
            "\tVal. Loss: 0.446 | Val. Acc: 0.798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 157/157 [00:15<00:00,  9.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Time: 0.0m 17.15s\n",
            "\tTrain Loss: 0.314 | Train Acc: 0.877\n",
            "\tVal. Loss: 0.529 | Val. Acc: 0.785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 157/157 [00:15<00:00,  9.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as LSTM_PyTorch_best.pt\n",
            "Epoch: 07 | Time: 0.0m 17.22s\n",
            "\tTrain Loss: 0.274 | Train Acc: 0.893\n",
            "\tVal. Loss: 0.339 | Val. Acc: 0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 157/157 [00:16<00:00,  9.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Time: 0.0m 17.48s\n",
            "\tTrain Loss: 0.206 | Train Acc: 0.924\n",
            "\tVal. Loss: 0.366 | Val. Acc: 0.857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 157/157 [00:15<00:00,  9.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Time: 0.0m 17.21s\n",
            "\tTrain Loss: 0.168 | Train Acc: 0.941\n",
            "\tVal. Loss: 0.349 | Val. Acc: 0.869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 157/157 [00:15<00:00,  9.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Time: 0.0m 17.23s\n",
            "\tTrain Loss: 0.138 | Train Acc: 0.953\n",
            "\tVal. Loss: 0.355 | Val. Acc: 0.873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 196/196 [00:06<00:00, 28.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate GRU model\n",
        "print(\"\\n=== Training GRU Model ===\")\n",
        "trained_gru_model, gru_history = train_model(\n",
        "    gru_model, train_loader, val_loader, criterion,\n",
        "    gru_optimizer, EPOCHS, \"GRU_PyTorch\"\n",
        ")\n",
        "gru_results = evaluate_model(trained_gru_model, test_loader, \"GRU_PyTorch\")\n",
        "all_results['GRU'] = gru_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p5azYd_pd_J",
        "outputId": "b32a154f-df2a-496d-a3e6-fb1632323041"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training GRU Model ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 157/157 [00:11<00:00, 14.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as GRU_PyTorch_best.pt\n",
            "Epoch: 01 | Time: 0.0m 12.01s\n",
            "\tTrain Loss: 0.670 | Train Acc: 0.577\n",
            "\tVal. Loss: 0.609 | Val. Acc: 0.673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 157/157 [00:11<00:00, 14.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as GRU_PyTorch_best.pt\n",
            "Epoch: 02 | Time: 0.0m 12.13s\n",
            "\tTrain Loss: 0.489 | Train Acc: 0.768\n",
            "\tVal. Loss: 0.388 | Val. Acc: 0.839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 157/157 [00:11<00:00, 14.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Time: 0.0m 12.13s\n",
            "\tTrain Loss: 0.295 | Train Acc: 0.880\n",
            "\tVal. Loss: 0.396 | Val. Acc: 0.837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 157/157 [00:11<00:00, 13.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as GRU_PyTorch_best.pt\n",
            "Epoch: 04 | Time: 0.0m 12.21s\n",
            "\tTrain Loss: 0.216 | Train Acc: 0.918\n",
            "\tVal. Loss: 0.274 | Val. Acc: 0.889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 157/157 [00:11<00:00, 13.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Time: 0.0m 12.20s\n",
            "\tTrain Loss: 0.146 | Train Acc: 0.947\n",
            "\tVal. Loss: 0.344 | Val. Acc: 0.885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 157/157 [00:11<00:00, 13.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Time: 0.0m 12.23s\n",
            "\tTrain Loss: 0.099 | Train Acc: 0.967\n",
            "\tVal. Loss: 0.341 | Val. Acc: 0.893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 157/157 [00:11<00:00, 13.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Time: 0.0m 12.27s\n",
            "\tTrain Loss: 0.063 | Train Acc: 0.980\n",
            "\tVal. Loss: 0.398 | Val. Acc: 0.889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 157/157 [00:11<00:00, 13.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Time: 0.0m 12.30s\n",
            "\tTrain Loss: 0.041 | Train Acc: 0.989\n",
            "\tVal. Loss: 0.431 | Val. Acc: 0.887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 157/157 [00:11<00:00, 13.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Time: 0.0m 12.31s\n",
            "\tTrain Loss: 0.030 | Train Acc: 0.991\n",
            "\tVal. Loss: 0.473 | Val. Acc: 0.880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 157/157 [00:11<00:00, 13.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Time: 0.0m 12.35s\n",
            "\tTrain Loss: 0.026 | Train Acc: 0.993\n",
            "\tVal. Loss: 0.533 | Val. Acc: 0.871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 196/196 [00:04<00:00, 41.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare models\n",
        "model_names = list(all_results.keys())\n",
        "metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
        "comparison_data = {}\n",
        "\n",
        "for metric in metrics:\n",
        "    comparison_data[metric] = [all_results[model][metric] for model in model_names]\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data, index=model_names)\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "print(comparison_df)\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(14, 8))\n",
        "comparison_df.plot(kind='bar', figsize=(12, 6))\n",
        "plt.title('PyTorch Model Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Model')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.savefig(\"model_comparison_pytorch.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot ROC curves for all models\n",
        "plt.figure(figsize=(10, 8))\n",
        "for model_name in model_names:\n",
        "    plt.plot(\n",
        "        all_results[model_name]['fpr'],\n",
        "        all_results[model_name]['tpr'],\n",
        "        lw=2,\n",
        "        label=f'{model_name} (AUC = {all_results[model_name][\"auc\"]:.3f})'\n",
        "    )\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tingkat Positif Palsu (False Positive Rate)')\n",
        "plt.ylabel('Tingkat Positif Benar (True Positive Rate)')\n",
        "plt.title('Perbandingan Kurva ROC (PyTorch)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.savefig(\"roc_curves_comparison_pytorch.png\")\n",
        "plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "YxbaN8swpipA",
        "outputId": "0fe476eb-5ea1-4736-9f4e-4cae85c22a95"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Model Comparison ===\n",
            "      accuracy  precision   recall        f1       auc\n",
            "RNN    0.75516   0.734885  0.79832  0.765290  0.821134\n",
            "LSTM   0.85740   0.870409  0.83984  0.854851  0.930569\n",
            "GRU    0.85672   0.919631  0.78176  0.845109  0.940992\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Penjelasan Matematika Model RNN, LSTM, dan GRU\n",
        "\n",
        "Dokumen ini memberikan penjelasan matematika dari tiga arsitektur jaringan saraf berulang yang digunakan dalam tugas analisis sentimen: RNN Sederhana (Simple RNN), LSTM (Long Short-Term Memory), dan GRU (Gated Recurrent Unit).\n",
        "\n",
        "## 1. RNN Sederhana (Simple RNN)\n",
        "\n",
        "### Unit Berulang Dasar\n",
        "RNN Sederhana memproses sekuens input dan mempertahankan keadaan tersembunyi (hidden state) yang diperbarui pada setiap langkah waktu. Untuk setiap langkah waktu $t$, keadaan tersembunyi $h_t$ dihitung sebagai:\n",
        "\n",
        "$$h_t = \\sigma(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$\n",
        "\n",
        "Dimana:\n",
        "- $x_t$ adalah input pada langkah waktu $t$\n",
        "- $h_{t-1}$ adalah keadaan tersembunyi dari langkah waktu sebelumnya\n",
        "- $W_{xh}$ adalah matriks bobot untuk koneksi input-ke-tersembunyi\n",
        "- $W_{hh}$ adalah matriks bobot untuk koneksi berulang tersembunyi-ke-tersembunyi\n",
        "- $b_h$ adalah term bias\n",
        "- $\\sigma$ adalah fungsi aktivasi (biasanya tanh atau ReLU)\n",
        "\n",
        "Untuk output, kita menggunakan keadaan tersembunyi terakhir:\n",
        "\n",
        "$$y = \\sigma(W_{hy} h_T + b_y)$$\n",
        "\n",
        "Dimana:\n",
        "- $h_T$ adalah keadaan tersembunyi pada langkah waktu terakhir\n",
        "- $W_{hy}$ adalah matriks bobot untuk koneksi tersembunyi-ke-output\n",
        "- $b_y$ adalah bias output\n",
        "- $\\sigma$ adalah fungsi aktivasi (sigmoid untuk klasifikasi biner)\n",
        "\n",
        "### Masalah Gradien yang Menghilang/Meledak\n",
        "RNN sederhana menderita masalah gradien yang menghilang atau meledak selama backpropagation melalui waktu. Saat menghitung gradien, kita mengalikan dengan $W_{hh}$ berulang kali:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial h_{t+1}} \\cdot \\frac{\\partial h_{t+1}}{\\partial h_t} = \\frac{\\partial L}{\\partial h_{t+1}} \\cdot W_{hh} \\cdot \\text{diag}(\\sigma'(W_{xh} x_t + W_{hh} h_{t-1} + b_h))$$\n",
        "\n",
        "Jika nilai eigen dari $W_{hh}$ kurang dari 1, gradien menghilang pada sekuens panjang. Jika lebih besar dari 1, gradien meledak.\n",
        "\n",
        "## 2. LSTM (Long Short-Term Memory)\n",
        "\n",
        "LSTM menyelesaikan masalah gradien yang menghilang dengan memperkenalkan keadaan sel dan tiga mekanisme gerbang:\n",
        "\n",
        "### Gerbang dan Keadaan Sel\n",
        "Untuk setiap langkah waktu $t$:\n",
        "\n",
        "1. **Gerbang Lupa (Forget Gate)**: Memutuskan informasi apa yang akan dibuang dari keadaan sel\n",
        "   $$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
        "\n",
        "2. **Gerbang Input**: Memutuskan informasi baru apa yang akan disimpan dalam keadaan sel\n",
        "   $$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
        "   $$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
        "\n",
        "3. **Pembaruan Keadaan Sel**: Memperbarui keadaan sel lama menjadi keadaan sel baru\n",
        "   $$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
        "\n",
        "4. **Gerbang Output**: Memutuskan bagian mana dari keadaan sel yang akan dioutputkan\n",
        "   $$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
        "   $$h_t = o_t \\odot \\tanh(C_t)$$\n",
        "\n",
        "Dimana:\n",
        "- $\\odot$ merepresentasikan perkalian elemen-wise\n",
        "- $[h_{t-1}, x_t]$ merepresentasikan penggabungan keadaan tersembunyi sebelumnya dan input saat ini\n",
        "- $\\sigma$ adalah fungsi aktivasi sigmoid\n",
        "- $W_f, W_i, W_C, W_o$ adalah matriks bobot\n",
        "- $b_f, b_i, b_C, b_o$ adalah term bias\n",
        "\n",
        "Keadaan sel $C_t$ bertindak sebagai jalan raya yang dapat membawa informasi melintasi banyak langkah waktu dengan perubahan minimal, memungkinkan jaringan untuk mempelajari dependensi jangka panjang.\n",
        "\n",
        "### LSTM Dua Arah (Bidirectional LSTM)\n",
        "\n",
        "Dalam implementasi kita, kita menggunakan LSTM dua arah yang memproses sekuens input dalam arah maju dan mundur:\n",
        "\n",
        "$$\\overrightarrow{h_t} = \\text{LSTM}_{\\text{maju}}(x_t, \\overrightarrow{h_{t-1}})$$\n",
        "$$\\overleftarrow{h_t} = \\text{LSTM}_{\\text{mundur}}(x_t, \\overleftarrow{h_{t+1}})$$\n",
        "$$h_t = [\\overrightarrow{h_t}, \\overleftarrow{h_t}]$$\n",
        "\n",
        "Ini memungkinkan model untuk menangkap konteks dari kedua keadaan masa lalu dan masa depan untuk langkah waktu tertentu.\n",
        "\n",
        "## 3. GRU (Gated Recurrent Unit)\n",
        "\n",
        "GRU adalah versi yang disederhanakan dari LSTM dengan parameter yang lebih sedikit:\n",
        "\n",
        "### Gerbang Pembaruan dan Reset\n",
        "\n",
        "Untuk setiap langkah waktu $t$:\n",
        "\n",
        "1. **Gerbang Pembaruan (Update Gate)**: Memutuskan berapa banyak informasi masa lalu yang akan disimpan\n",
        "   $$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)$$\n",
        "\n",
        "2. **Gerbang Reset**: Memutuskan berapa banyak informasi masa lalu yang akan dilupakan\n",
        "   $$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)$$\n",
        "\n",
        "3. **Keadaan Tersembunyi Kandidat**: Menghitung keadaan tersembunyi kandidat\n",
        "   $$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)$$\n",
        "\n",
        "4. **Pembaruan Keadaan Tersembunyi**: Memperbarui keadaan tersembunyi\n",
        "   $$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
        "\n",
        "Dimana:\n",
        "- $\\odot$ merepresentasikan perkalian elemen-wise\n",
        "- $[h_{t-1}, x_t]$ merepresentasikan penggabungan keadaan tersembunyi sebelumnya dan input saat ini\n",
        "- $\\sigma$ adalah fungsi aktivasi sigmoid\n",
        "- $W_z, W_r, W_h$ adalah matriks bobot\n",
        "- $b_z, b_r, b_h$ adalah term bias\n",
        "\n",
        "GRU menggabungkan gerbang lupa dan input menjadi satu gerbang pembaruan, dan menggabungkan keadaan sel dan keadaan tersembunyi. Ini membuatnya lebih efisien secara komputasi sambil tetap mengatasi masalah gradien yang menghilang.\n",
        "\n",
        "## 4. Metrik Evaluasi\n",
        "\n",
        "### Loss Binary Cross-Entropy\n",
        "Untuk klasifikasi biner, kita menggunakan loss binary cross-entropy:\n",
        "\n",
        "$$L = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$\n",
        "\n",
        "Dimana:\n",
        "- $N$ adalah jumlah sampel\n",
        "- $y_i$ adalah label sebenarnya (0 atau 1)\n",
        "- $\\hat{y}_i$ adalah probabilitas yang diprediksi\n",
        "\n",
        "### Akurasi (Accuracy)\n",
        "Proporsi instance yang diklasifikasikan dengan benar:\n",
        "\n",
        "$$\\text{Akurasi} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$$\n",
        "\n",
        "### Presisi (Precision)\n",
        "Proporsi prediksi positif benar di antara semua prediksi positif:\n",
        "\n",
        "$$\\text{Presisi} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n",
        "\n",
        "### Recall\n",
        "Proporsi prediksi positif benar di antara semua positif aktual:\n",
        "\n",
        "$$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
        "\n",
        "### F1-Score\n",
        "Rata-rata harmonik dari presisi dan recall:\n",
        "\n",
        "$$\\text{F1} = 2 \\times \\frac{\\text{Presisi} \\times \\text{Recall}}{\\text{Presisi} + \\text{Recall}}$$\n",
        "\n",
        "### AUC-ROC\n",
        "Area di Bawah Kurva Karakteristik Operasi Penerima (Receiver Operating Characteristic). Kurva ROC memplot Tingkat Positif Benar (Recall) terhadap Tingkat Positif Palsu:\n",
        "\n",
        "$$\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
        "$$\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}$$\n",
        "\n",
        "AUC mengukur area di bawah kurva ini, dengan nilai yang lebih tinggi menunjukkan diskriminasi yang lebih baik.\n",
        "\n",
        "## 5. Fitur Model Tambahan\n",
        "\n",
        "### Lapisan Embedding\n",
        "Mentransformasi indeks kata menjadi vektor padat:\n",
        "\n",
        "$$e_t = W_e x_t$$\n",
        "\n",
        "Dimana:\n",
        "- $x_t$ adalah vektor one-hot encoded atau indeks\n",
        "- $W_e$ adalah matriks embedding\n",
        "- $e_t$ adalah vektor embedding yang dihasilkan\n",
        "\n",
        "### Normalisasi Lapisan (Layer Normalization)\n",
        "Menormalkan aktivasi dari lapisan sebelumnya untuk memiliki rata-rata nol dan varians satu:\n",
        "\n",
        "$$\\mu = \\frac{1}{H} \\sum_{i=1}^{H} a_i$$\n",
        "$$\\sigma^2 = \\frac{1}{H} \\sum_{i=1}^{H} (a_i - \\mu)^2$$\n",
        "$$\\hat{a}_i = \\frac{a_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
        "$$b_i = \\gamma \\hat{a}_i + \\beta$$\n",
        "\n",
        "Dimana:\n",
        "- $a_i$ adalah aktivasi dari neuron ke-i\n",
        "- $H$ adalah jumlah neuron dalam lapisan\n",
        "- $\\gamma$ dan $\\beta$ adalah parameter yang dipelajari\n",
        "- $\\epsilon$ adalah nilai kecil untuk stabilitas numerik"
      ],
      "metadata": {
        "id": "fWHgdriiqap-"
      }
    }
  ]
}